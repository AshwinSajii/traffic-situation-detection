{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSwjpDYNncCI"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd #\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import random\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "8XIx229ln6UE",
        "outputId": "c7a378cb-85a8-42ff-817d-934c7384d0c7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'traffic detection.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-563710513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"traffic detection.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'traffic detection.xlsx'"
          ]
        }
      ],
      "source": [
        "\n",
        "data = pd.read_excel(\"traffic detection.xlsx\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAGqR8WypPOe"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3kLdze-pyR4"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwbSwss_p01s"
      },
      "outputs": [],
      "source": [
        "data.isnull().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq0TmY1GqIVQ"
      },
      "source": [
        "there are no null values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zvpTzAxp3fl"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ARzBVG1smr"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzwPQfpP1w6C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.swarmplot(y=data[\"total\"], x = data[\"Traffic Situation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L6SSpuD15BJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.swarmplot(y=data[\"CarCount\"], x = data[\"Traffic Situation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjpab7OY4fDK"
      },
      "source": [
        "Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf_LMZBn4M8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "data[\"Day\"] = encoder.fit_transform(data[\"Day of the week\"])\n",
        "data[\"Traffic_Situation\"] = encoder.fit_transform(data[\"Traffic Situation\"])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48wASdL84qzn"
      },
      "source": [
        "dropping the not required columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fzApD6L4uYv"
      },
      "outputs": [],
      "source": [
        "data.drop([\"Day of the week\",\"Traffic Situation\"],inplace = True, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgrUUmFb40Vt"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dqXPtkU431r"
      },
      "outputs": [],
      "source": [
        "data['Time'] = pd.to_datetime(data['Time'].astype(str))\n",
        "\n",
        "# Extract the hour and minute components and convert 'Time' to minutes since midnight\n",
        "data['Time'] = data['Time'].dt.hour * 60 + data['Time'].dt.minute\n",
        "\n",
        "# Convert 'Time' to hours\n",
        "data['Time'] = data['Time'] / 60.0\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tpfDofFP7GF"
      },
      "source": [
        "outlier detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DCr39kW7OoW"
      },
      "outputs": [],
      "source": [
        "# Selecting the columns of interest\n",
        "columns_of_interest = ['CarCount', 'BikeCount', 'TruckCount', 'suv count', 'total']\n",
        "\n",
        "# Plot boxplots for outlier detection\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, column in enumerate(columns_of_interest, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    data.boxplot(column=column)\n",
        "    plt.title(f'Boxplot for {column}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hFkBTrhRSmY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "Q1=np.percentile(data['BikeCount'],25, interpolation = 'midpoint')\n",
        "Q2=np.percentile(data['BikeCount'],50, interpolation = 'midpoint')\n",
        "Q3=np.percentile(data['BikeCount'],75, interpolation = 'midpoint')\n",
        "Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae6GLxzTTZQi"
      },
      "outputs": [],
      "source": [
        "IQR=Q3-Q1\n",
        "IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhrTVolcTcYp"
      },
      "outputs": [],
      "source": [
        "lowlim=Q1-1.5*IQR\n",
        "upplim=Q3+1.5*IQR\n",
        "print(lowlim)\n",
        "print(upplim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BDwSoznTgaw"
      },
      "outputs": [],
      "source": [
        "outliers = []\n",
        "for i in data['BikeCount']:\n",
        "    if i > upplim or i < lowlim:\n",
        "        outliers.append(i)\n",
        "\n",
        "print(outliers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUJknTMgTi_R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "Q1=np.percentile(data['TruckCount'],25, interpolation = 'midpoint')\n",
        "Q2=np.percentile(data['TruckCount'],50, interpolation = 'midpoint')\n",
        "Q3=np.percentile(data['TruckCount'],75, interpolation = 'midpoint')\n",
        "Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM_HUSxkT4_v"
      },
      "outputs": [],
      "source": [
        "IQR=Q3-Q1\n",
        "IQR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWt52sRQT82c"
      },
      "outputs": [],
      "source": [
        "lowlim=Q1-1.5*IQR\n",
        "upplim=Q3+1.5*IQR\n",
        "print(lowlim)\n",
        "print(upplim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hgugp8BT_U-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def handle_outliers_iqr(data, multiplier=1.5):\n",
        "    # Calculate the first and third quartiles (Q1 and Q3)\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "\n",
        "    # Calculate the IQR (Interquartile Range)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the lower and upper bounds to identify outliers\n",
        "    lowlim = Q1 - multiplier * IQR\n",
        "    upplim = Q3 + multiplier * IQR\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = (data < lowlim) | (data > upplim)\n",
        "\n",
        "    # Handle outliers (you can choose a method like removing, replacing, or transforming)\n",
        "    # In this example, we replace outliers with the median value\n",
        "    data[outliers] = np.median(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "data = np.array([54, 59, 57, 58, 56, 53, 55, 57, 54, 53, 53, 60, 55, 57, 57, 57, 57, 58, 57, 56, 53, 59, 58, 56])\n",
        "print(\"Original data:\", data)\n",
        "\n",
        "# Handle outliers using IQR method\n",
        "data_handled = handle_outliers_iqr(data)\n",
        "\n",
        "print(\"Data after handling outliers:\", data_handled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiiVYdrtn4fP"
      },
      "source": [
        "#minmaxscaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0sbGo6Wbn7_"
      },
      "outputs": [],
      "source": [
        "print(type(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q24_FvUab7D1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the NumPy array to a Pandas DataFrame\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "# Now you can access columns\n",
        "print(data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuvJzJxjbpxj"
      },
      "outputs": [],
      "source": [
        "print(data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7CoGlgsci8Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming data is a DataFrame or a NumPy array converted to a DataFrame\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "# Check the available column names\n",
        "print(data.columns)\n",
        "\n",
        "# If 'TruckCount' is not in the columns, investigate the correct column names\n",
        "# Correct the column names if needed and then apply pd.to_numeric\n",
        "data['BikeCount'] = pd.to_numeric(data['BikeCount'], errors='coerce')\n",
        "\n",
        "# Check again after correcting column names\n",
        "print(data.columns)\n",
        "\n",
        "# Now apply pd.to_numeric for 'TruckCount'\n",
        "data['TruckCount'] = pd.to_numeric(data['TruckCount'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIq6yRPXcBvz"
      },
      "outputs": [],
      "source": [
        "data['BikeCount'] = pd.to_numeric(data['BikeCount'], errors='coerce')\n",
        "data['TruckCount'] = pd.to_numeric(data['TruckCount'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGuK09xrUILR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Select columns to be scaled\n",
        "columns_to_scale = ['BikeCount','TruckCount']\n",
        "# Fit and transform the selected columns\n",
        "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "# Check the updated DataFrame\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrKcE3w6xeMi"
      },
      "source": [
        "#Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZQGbPIC25-G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel(\"traffic detection.xlsx\")\n",
        "\n",
        "# Label Encoding\n",
        "encoder = LabelEncoder()\n",
        "data[\"Day\"] = encoder.fit_transform(data[\"Day of the week\"])\n",
        "data[\"Traffic_Situation\"] = encoder.fit_transform(data[\"Traffic Situation\"])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data.drop([\"Day of the week\", \"Traffic Situation\"], inplace=True, axis=1)\n",
        "\n",
        "# Convert 'Time' to hours\n",
        "data['Time'] = pd.to_datetime(data['Time'].astype(str))\n",
        "data['Hour'] = data['Time'].dt.hour\n",
        "data['Minute'] = data['Time'].dt.minute\n",
        "data['Time'] = data['Hour'] * 60 + data['Minute']\n",
        "data.drop(['Hour', 'Minute'], inplace=True, axis=1)\n",
        "\n",
        "# Min-max scaling\n",
        "scaler = MinMaxScaler()\n",
        "columns_to_scale = ['BikeCount', 'TruckCount']\n",
        "data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "features = data.drop(['Traffic_Situation'], axis=1)\n",
        "target = data['Traffic_Situation']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Drop datetime columns if any\n",
        "X_train = X_train.select_dtypes(exclude=['datetime64'])\n",
        "X_test = X_test.select_dtypes(exclude=['datetime64'])\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "model1 = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "model1.fit(X_train, y_train)\n",
        "predictions = model1.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = (accuracy_score(y_test, predictions)*100)\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJXyk4Un7Gfp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
